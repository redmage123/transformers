{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying Transformer Neural Networks and the Attention Mechanism\n",
    "\n",
    "## Introduction\n",
    "1. Brief introduction to deep learning and neural networks.\n",
    "2. Limitations of traditional Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) for natural language processing tasks.\n",
    "3. Introducing the Transformer architecture and attention mechanism as a solution to these limitations.\n",
    "\n",
    "## Section 1: The Transformer Architecture\n",
    "\n",
    "### 1.1. Overview of the Transformer architecture\n",
    "- Key components of the architecture: encoder, decoder, multi-head self-attention, and positional encoding.\n",
    "\n",
    "### 1.2. Encoder\n",
    "- Role of the encoder in the Transformer architecture.\n",
    "- Explanation of self-attention, its calculation, and its role in the encoding process.\n",
    "- Description of layer normalization, feed-forward layers, and residual connections.\n",
    "\n",
    "### 1.3. Decoder\n",
    "- Purpose of the decoder and its structure.\n",
    "- Explanation of masked self-attention mechanism in the decoder to prevent information leakage.\n",
    "- Final linear and softmax layers for generating output probabilities.\n",
    "\n",
    "## Section 2: The Attention Mechanism\n",
    "\n",
    "### 2.1. Concept and motivation\n",
    "- Idea behind the attention mechanism and its purpose in the context of sequence-to-sequence models.\n",
    "- Limitations of fixed-length context vectors in RNN-based models leading to the development of the attention mechanism.\n",
    "\n",
    "### 2.2. Types of attention mechanisms\n",
    "- Description of global (soft) attention and local (hard) attention.\n",
    "- Advantages and disadvantages of each type.\n",
    "\n",
    "### 2.3. Multi-head attention\n",
    "- Concept of multi-head attention and its benefits in capturing different types of relationships between words in a sequence.\n",
    "- Process of splitting, concatenating, and linearly projecting attention heads.\n",
    "\n",
    "## Section 3: Applications and Advancements\n",
    "\n",
    "### 3.1. Natural language processing applications\n",
    "- Popular NLP tasks that benefit from Transformer architectures, such as machine translation, text summarization, and sentiment analysis.\n",
    "\n",
    "### 3.2. Beyond text: Vision and multimodal tasks\n",
    "- Extension of Transformer architectures to computer vision tasks, such as image classification and object detection.\n",
    "- Application of Transformers to multimodal tasks, such as visual question answering and image-captioning.\n",
    "\n",
    "### 3.3. Notable Transformer models\n",
    "- Brief description of famous Transformer models, such as BERT, GPT, T5, and OpenAI's ChatGPT.\n",
    "- Respective advancements and applications of these models.\n",
    "\n",
    "## Conclusion\n",
    "1. Summary of the main points discussed in the presentation.\n",
    "2. Impact of the Transformer architecture and attention mechanism on the field of AI.\n",
    "3. Future research directions and potential improvements in Transformer-based models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### 1.1. Deep learning and neural networks\n",
    "- Definition of deep learning and neural networks.\n",
    "- Basic components: neurons, layers, and activation functions.\n",
    "- Types of neural networks: feedforward, recurrent, and convolutional.\n",
    "\n",
    "### 1.2. Limitations of RNNs and CNNs in NLP\n",
    "- RNNs: vanishing and exploding gradients, difficulty handling long-range dependencies, and sequential processing.\n",
    "- CNNs: limited receptive field, difficulty capturing long-range dependencies, and suboptimal for sequence-to-sequence tasks.\n",
    "\n",
    "### 1.3. Introduction to Transformer architecture and attention mechanism\n",
    "- Brief overview of the Transformer architecture as an alternative to RNNs and CNNs for NLP tasks.\n",
    "- Introduction to the attention mechanism as a key component of the Transformer architecture.\n",
    "- Explanation of how the attention mechanism overcomes the limitations of RNNs and CNNs in NLP.\n",
    "\n",
    "## Section 1: The Transformer Architecture\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1. Deep Learning: A Comprehensive Overview\n",
    "\n",
    "Deep learning is a subfield of machine learning that focuses on training multi-layered artificial neural networks to automatically learn hierarchical representations of data. By leveraging these hierarchical representations, deep learning models can identify and extract complex patterns, enabling them to make data-driven predictions or decisions. Deep learning has been particularly transformative in fields such as computer vision, natural language processing, speech recognition, and reinforcement learning.\n",
    "\n",
    "#### Key Characteristics of Deep Learning:\n",
    "\n",
    "1. **Neural networks with multiple layers**: Deep learning models consist of multiple layers of interconnected neurons, which allow them to learn hierarchical feature representations. The depth of the network refers to the number of layers, with deeper networks generally having greater representational capacity.\n",
    "2. **Representation learning**: Deep learning models automatically learn to extract useful features from raw data, eliminating the need for manual feature engineering. This ability to learn feature hierarchies is a key advantage of deep learning over traditional machine learning methods.\n",
    "3. **End-to-end learning**: Many deep learning models can be trained end-to-end, learning to map raw inputs directly to desired outputs. This approach reduces the need for pre-processing and feature extraction, simplifying the overall learning pipeline.\n",
    "4. **Non-linear activation functions**: Non-linear activation functions, such as ReLU, sigmoid, or tanh, are applied to the output of each neuron. These non-linearities allow deep learning models to learn and represent complex, non-linear relationships between inputs and outputs.\n",
    "5. **Parameter optimization**: Deep learning models involve a large number of parameters (weights and biases) that are optimized during training using techniques like gradient descent or its variants (e.g., stochastic gradient descent, Adam, RMSprop). The optimization process minimizes a predefined loss function that measures the difference between the model's predictions and the ground-truth labels.\n",
    "6. **Regularization techniques**: To prevent overfitting and improve generalization, deep learning models often employ regularization techniques such as dropout, weight decay (L2 regularization), or early stopping. These methods help constrain the model's capacity and prevent it from learning the noise in the training data.\n",
    "7. **Large-scale data and computational requirements**: Deep learning models typically require large amounts of labeled data for training, as they have a high capacity to learn complex patterns. Additionally, training deep learning models is computationally intensive, often requiring specialized hardware like GPUs or TPUs.\n",
    "\n",
    "### Popular Deep Learning Architectures\n",
    "\n",
    "1. **Feedforward Neural Networks (FNNs)**: The simplest type of neural network, where information flows in one direction from input to output, with no loops or cycles. FNNs are often used for basic classification and regression tasks.\n",
    "\n",
    "\n",
    "2. **Convolutional Neural Networks (CNNs)**: CNNs are primarily used for image classification, object detection, and computer vision tasks. They utilize convolutional layers to automatically learn hierarchical feature representations from input images. Some popular CNN architectures include:\n",
    "\n",
    "    - LeNet-5\n",
    "    - AlexNet\n",
    "    - VGGNet (VGG-16, VGG-19)\n",
    "    - Inception (GoogLeNet, Inception-v3)\n",
    "    - ResNet (ResNet-50, ResNet-101)\n",
    "    - DenseNet\n",
    "    - MobileNet\n",
    "\n",
    "3. **Recurrent Neural Networks (RNNs)**: RNNs are designed to handle sequential data, such as time series or natural language. They maintain a hidden state that can capture information from previous time steps, allowing them to model temporal dependencies. Some popular RNN architectures include:\n",
    "\n",
    "    - Vanilla RNN\n",
    "    - Long Short-Term Memory (LSTM)\n",
    "    - Gated Recurrent Unit (GRU)\n",
    "    - Bidirectional RNN\n",
    "    - Bidirectional LSTM/GRU\n",
    "\n",
    "4. **Transformers**: Transformers are a type of neural network architecture designed for handling sequential data, particularly in natural language processing tasks. They leverage self-attention mechanisms to model long-range dependencies and process input sequences in parallel, rather than sequentially as in RNNs. Some popular Transformer architectures include:\n",
    "\n",
    "    - Original Transformer (Vaswani et al., 2017)\n",
    "    - BERT (Bidirectional Encoder Representations from Transformers)\n",
    "    - GPT (Generative Pre-trained Transformer)\n",
    "    - T5 (Text-to-Text Transfer Transformer)\n",
    "    - RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
    "    - DistilBERT (a distilled version of BERT)\n",
    "\n",
    "5. **Autoencoders (AEs)**: Autoencoders are unsupervised learning models used for dimensionality reduction, feature learning, and generative tasks. They consist of an encoder that maps input data to a lower-dimensional latent space and a decoder that reconstructs the original input from the latent representation. Some popular autoencoder architectures include:\n",
    "\n",
    "    - Vanilla Autoencoder\n",
    "    - Sparse Autoencoder\n",
    "    - Denoising Autoencoder\n",
    "    - Variational Autoencoder (VAE)\n",
    "\n",
    "6. **Generative Adversarial Networks (GANs)**: GANs are a class of generative models that learn to generate realistic samples from a given distribution. They consist of a generator network that creates fake samples and a discriminator network that distinguishes between real and fake samples. The generator and discriminator are trained simultaneously in a two-player adversarial game. Some popular GAN architectures include:\n",
    "\n",
    "    - Vanilla GAN\n",
    "    - Deep Convolutional GAN (DCGAN)\n",
    "    - Wasserstein GAN (WGAN)\n",
    "    - Conditional GAN (cGAN)\n",
    "    - StyleGAN (Style-Based Generator Architecture for GANs)\n",
    "\n",
    "These architectures have been widely used and adapted for various applications in computer vision, natural language processing, speech recognition, and other domains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of a neural network:\n",
    "\n",
    "1. **Input layer**: This is the first layer of the neural network that receives input data from external sources. The input layer has as many neurons as there are features or dimensions in the input data.\n",
    "\n",
    "2. **Hidden layers**: Hidden layers are the layers between the input and output layers. They are responsible for processing the input data and learning complex patterns and representations. Hidden layers can vary in number and size, depending on the architecture of the neural network and the complexity of the task.\n",
    "\n",
    "3. **Output layer**: This is the final layer of the neural network that produces the output or prediction. The output layer has as many neurons as there are classes or target variables in the problem. For regression tasks, the output layer usually has a single neuron, while for classification tasks, it has one neuron per class.\n",
    "\n",
    "4. **Neurons**: Neurons, or nodes, are the basic processing units of a neural network. Each neuron receives input from the neurons in the previous layer, applies a linear transformation (weighted sum) and a non-linear activation function, and sends the output to the neurons in the next layer.\n",
    "\n",
    "5. **Weights and biases**: Weights and biases are the learnable parameters of a neural network. Weights are the connection strengths between neurons, while biases are additional terms added to the weighted sum in each neuron. During training, the neural network adjusts the weights and biases to minimize the error between its predictions and the ground truth.\n",
    "\n",
    "6. **Activation functions**: Activation functions introduce non-linearity into the neural network, enabling it to learn complex, non-linear relationships between input and output variables. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh (hyperbolic tangent).\n",
    "\n",
    "7. **Loss function**: The loss function quantifies the difference between the predictions made by the neural network and the actual ground truth labels. It is used during training to update the weights and biases of the network. Common loss functions include mean squared error for regression tasks and cross-entropy for classification tasks.\n",
    "\n",
    "8. **Optimizer**: The optimizer is an algorithm used to update the weights and biases of the neural network to minimize the loss function. Optimizers are based on gradient descent, with variants such as stochastic gradient descent (SGD), momentum, AdaGrad, RMSProp, and Adam.\n",
    "\n",
    "9. **Backpropagation**: Backpropagation is the process of computing the gradient of the loss function with respect to each weight and bias by applying the chain rule. It is a crucial step in training a neural network, as it allows the optimizer to update the weights and biases to minimize the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs) and Variants\n",
    "\n",
    "### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network specifically designed to handle sequential data, such as time series or natural language. Unlike feedforward networks, RNNs maintain a hidden state that can capture information from previous time steps, allowing them to model temporal dependencies in the data. The hidden state is updated at each time step by incorporating both the current input and the hidden state from the previous time step:\n",
    "\n",
    "$h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$\n",
    "\n",
    "where $h_t$ is the hidden state at time step $t$, $x_t$ is the input at time step $t$, $W_{xh}$ and $W_{hh}$ are the input-to-hidden and hidden-to-hidden weight matrices, respectively, $b_h$ is the bias term, and $f$ is a non-linear activation function, such as the hyperbolic tangent (tanh).\n",
    "\n",
    "However, vanilla RNNs suffer from the vanishing gradient problem, which makes it difficult for them to learn long-range dependencies in the data. This issue has led to the development of more advanced RNN variants, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a rolled up RNN\n",
    "<img src=\"transformers/docs/graphics/RNN-rolled.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here it is unrolled: \n",
    "<img src=\"transformers/docs/graphics/RNN-unrolled.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How RNN's actually work ##\n",
    "An RNN works like this; First words get transformed into machine-readable vectors. Then the RNN processes the sequence of vectors one by one.\n",
    "Here's an illustrated visual showing how RNN's actually work:\n",
    "\n",
    "<img src=\"graphics/RNN-1.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While processing, it passes the previous hidden state to the next step of the sequence. The hidden state acts as the neural networks memory. It holds information on previous data the network has seen before.\n",
    "<img src=\"graphics/RNN-2.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with RNN's\n",
    "\n",
    "### The Vanishing Gradient\n",
    "The vanishing gradient problem is an issue that arises during the training of Recurrent Neural Networks (RNNs) and other deep learning models. It occurs when the gradients of the loss function with respect to the model parameters become very small, causing the model's weights to update very slowly or not at all. This problem can lead to poor training performance and make it difficult for the model to learn long-range dependencies in the data.\n",
    "\n",
    "In RNNs, the vanishing gradient problem is particularly pronounced due to the recurrent nature of the network. During backpropagation through time (BPTT), gradients are computed for each time step by considering the dependencies between the current time step and all previous time steps. As the sequence length increases, the gradients can become very small or very large, leading to the vanishing or exploding gradient problem, respectively.\n",
    "\n",
    "The vanishing gradient problem in RNNs can be attributed to the chain rule of derivatives, which is used during the backpropagation process. When calculating the gradient of the loss function with respect to a weight at a certain time step, the chain rule requires the multiplication of partial derivatives across all time steps. If the values of these partial derivatives are small (less than 1), their product will become exponentially smaller as the sequence length increases, causing the gradient to vanish.\n",
    "\n",
    "In mathematical terms, let's consider an RNN with a simple activation function like the hyperbolic tangent (tanh). The gradient of the loss function with respect to the hidden state at time step $t$ is given by:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L}{\\partial h_{t+1}} \\frac{\\partial h_{t+1}}{\\partial h_t}$\n",
    "\n",
    "Since $\\frac{\\partial h_{t+1}}{\\partial h_t}$ involves the weight matrix and the derivative of the activation function, if the activation function's derivative has values less than 1, multiplying these small values across multiple time steps will cause the gradient to vanish.\n",
    "\n",
    "The vanishing gradient problem makes it difficult for RNNs to learn long-range dependencies and capture information from earlier time steps. To address this issue, more advanced RNN architectures, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks, have been developed. These architectures introduce gating mechanisms that can control the flow of information through the network, allowing gradients to flow more freely across long sequences and mitigating the vanishing gradient problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at a cell of the RNN to see how you would calculate the hidden state. First, the input and previous hidden state are combined to form a vector. That vector now has information on the current input and previous inputs. The vector goes through the tanh activation, and the output is the new hidden state, or the memory of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphics/rnn-3.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh activation\n",
    "The tanh activation is used to help regulate the values flowing through the network. The tanh function squishes values to always be between -1 and 1.\n",
    "\n",
    "<img src=\"graphics/RNN-4.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The exploding gradient problem\n",
    "\n",
    "When vectors are flowing through a neural network, it undergoes many transformations due to various math operations. So imagine a value that continues to be multiplied by let’s say 3. You can see how some values can explode and become astronomical, causing other values to seem insignificant.\n",
    "\n",
    "<img src=\"graphics/RNN-5.gif\"/>\n",
    "\n",
    "\n",
    "In RNNs, the exploding gradient problem can be particularly pronounced due to the recurrent nature of the network. During backpropagation through time (BPTT), gradients are computed for each time step by considering the dependencies between the current time step and all previous time steps. As the sequence length increases, the gradients can become very small (vanishing gradient problem) or very large (exploding gradient problem).\n",
    "\n",
    "The exploding gradient problem in RNNs can also be attributed to the chain rule of derivatives, which is used during the backpropagation process. When calculating the gradient of the loss function with respect to a weight at a certain time step, the chain rule requires the multiplication of partial derivatives across all time steps. If the values of these partial derivatives are large (greater than 1), their product will become exponentially larger as the sequence length increases, causing the gradient to explode.\n",
    "\n",
    "In mathematical terms, let's consider an RNN with a simple activation function like the hyperbolic tangent (tanh). The gradient of the loss function with respect to the hidden state at time step $t$ is given by:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L}{\\partial h_{t+1}} \\frac{\\partial h_{t+1}}{\\partial h_t}$\n",
    "\n",
    "Since $\\frac{\\partial h_{t+1}}{\\partial h_t}$ involves the weight matrix and the derivative of the activation function, if the activation function's derivative or the weight matrix elements have values greater than 1, multiplying these large values across multiple time steps will cause the gradient to explode.\n",
    "\n",
    "The exploding gradient problem can lead to unstable training and cause the model to fail to learn the underlying patterns in the data. To address this issue, a common approach is to apply gradient clipping, which involves rescaling the gradients if their magnitude exceeds a predefined threshold. This prevents the gradients from becoming too large and helps stabilize the training process.\n",
    "\n",
    "It is worth noting that advanced RNN architectures like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks, which were designed to mitigate the vanishing gradient problem, can also help alleviate the exploding gradient problem to some extent due to their gating mechanisms and improved gradient flow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory (LSTM)\n",
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) designed to address the vanishing and exploding gradient problems encountered when training traditional RNNs. LSTMs are particularly effective at learning long-range dependencies in sequential data, making them suitable for various tasks, such as natural language processing, time series prediction, and speech recognition.\n",
    "\n",
    "An LSTM has a similar control flow as a recurrent neural network. It processes data passing on information as it propagates forward. The differences are the operations within the LSTM’s cells.\n",
    "\n",
    "The LSTM architecture introduces a memory cell and three gating mechanisms that control the flow of information within the network:\n",
    "\n",
    "1. **Input Gate**: This gate determines how much of the new input should be stored in the memory cell. It consists of a sigmoid activation function, which produces values between 0 and 1, representing the proportion of the new input to be stored.\n",
    "2. **Forget Gate**: This gate controls how much of the previous memory cell state should be retained or discarded. Similar to the input gate, it also uses a sigmoid activation function to produce values between 0 and 1, indicating the proportion of the previous memory cell state to retain.\n",
    "3. **Output Gate**: This gate determines how much of the memory cell's information should be used as the output of the LSTM unit. It uses a sigmoid activation function to produce values between 0 and 1, representing the proportion of the memory cell state to be used as the output.\n",
    "\n",
    "The memory cell state, along with these gating mechanisms, allows LSTMs to selectively store, update, and retrieve information over long sequences, mitigating the vanishing and exploding gradient issues faced by traditional RNNs.\n",
    "\n",
    "The equations governing an LSTM can be expressed as follows:\n",
    "\n",
    "1. Input gate: $i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i)$\n",
    "2. Forget gate: $f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f)$\n",
    "3. Output gate: $o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_o)$\n",
    "4. Memory cell state update: $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)$\n",
    "5. Hidden state update: $h_t = o_t \\odot \\tanh(c_t)$\n",
    "where:\n",
    "- $x_t$ is the input at time step $t$\n",
    "- $h_{t-1}$ is the hidden state at time step $t-1$\n",
    "- $c_{t-1}$ is the memory cell state at time step $t-1$\n",
    "- $i_t$, $f_t$, and $o_t$ are the input, forget, and output gates at time step $t$, respectively\n",
    "- $c_t$ and $h_t$ are the updated memory cell state and hidden state at time step $t$, respectively\n",
    "- $W$ and $b$ denote the weight matrices and bias vectors for each gate, respectively\n",
    "- $\\sigma$ is the sigmoid activation function\n",
    "- $\\odot$ denotes element-wise multiplication\n",
    "- $\\tanh$ is the hyperbolic tangent activation function\n",
    "\n",
    "This is a graphical representation of an LSTM cell.\n",
    "<img src=\"graphics/RNN-6.webp\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Concept\n",
    "The core concept of LSTM’s are the cell state, and it’s various gates. The cell state act as a transport highway that transfers relative information all the way down the sequence chain. You can think of it as the “memory” of the network. The cell state, in theory, can carry relevant information throughout the processing of the sequence. So even information from the earlier time steps can make it’s way to later time steps, reducing the effects of short-term memory. As the cell state goes on its journey, information get’s added or removed to the cell state via gates. The gates are different neural networks that decide which information is allowed on the cell state. The gates can learn what information is relevant to keep or forget during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Functions\n",
    "Gates contains sigmoid activations. A sigmoid activation is similar to the tanh activation. Instead of squishing values between -1 and 1, it squishes values between 0 and 1. That is helpful to update or forget data because any number getting multiplied by 0 is 0, causing values to disappears or be “forgotten.” Any number multiplied by 1 is the same value therefore that value stay’s the same or is “kept.” The network can learn which data is not important therefore can be forgotten or which data is important to keep.\n",
    "\n",
    "<img src=\"graphics/sigmoid-1.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forget gate ###\n",
    "First, we have the forget gate. This gate decides what information should be thrown away or kept. Information from the previous hidden state and information from the current input is passed through the sigmoid function. Values come out between 0 and 1. The closer to 0 means to forget, and the closer to 1 means to keep.\n",
    "\n",
    "<img src=\"graphics/forget-1.gif\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Gate ###\n",
    "To update the cell state, we have the input gate. First, we pass the previous hidden state and current input into a sigmoid function. That decides which values will be updated by transforming the values to be between 0 and 1. 0 means not important, and 1 means important. You also pass the hidden state and current input into the tanh function to squish values between -1 and 1 to help regulate the network. Then you multiply the tanh output with the sigmoid output. The sigmoid output will decide which information is important to keep from the tanh output.\n",
    "\n",
    "<img src=\"graphics/input-1.gif\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell State ###\n",
    "Now we should have enough information to calculate the cell state. First, the cell state gets pointwise multiplied by the forget vector. This has a possibility of dropping values in the cell state if it gets multiplied by values near 0. Then we take the output from the input gate and do a pointwise addition which updates the cell state to new values that the neural network finds relevant. That gives us our new cell state.\n",
    "\n",
    "<img src=\"graphics/cell-1.gif\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Gate ###\n",
    "Last we have the output gate. The output gate decides what the next hidden state should be. Remember that the hidden state contains information on previous inputs. The hidden state is also used for predictions. First, we pass the previous hidden state and the current input into a sigmoid function. Then we pass the newly modified cell state to the tanh function. We multiply the tanh output with the sigmoid output to decide what information the hidden state should carry. The output is the hidden state. The new cell state and the new hidden is then carried over to the next time step.\n",
    "\n",
    "<img src=\"graphics/output-1.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with LSTM networks ##\n",
    "\n",
    "Although Long Short-Term Memory (LSTM) networks have been successful in learning long-range dependencies and mitigating the vanishing and exploding gradient problems in recurrent neural networks (RNNs), they have their own set of limitations and challenges:\n",
    "\n",
    "1. **Complexity**: LSTMs have a more complex architecture than traditional RNNs, with multiple gates and a memory cell. This increased complexity results in a larger number of parameters to train, leading to higher computational requirements and longer training times.\n",
    "\n",
    "2. **Memory and Computational Requirements**: As mentioned earlier, the increased number of parameters in LSTMs results in higher memory and computational requirements. This can be a challenge, especially when dealing with large datasets and deep architectures. LSTMs may require specialized hardware, such as GPUs or TPUs, for efficient training.\n",
    "\n",
    "3. **Long Training Time**: Due to the complex nature of LSTMs and the larger number of parameters to train, training times can be quite long, especially for deep networks and large datasets. This can be a bottleneck for rapid prototyping and model iteration.\n",
    "\n",
    "4. **Difficulty in Parallelizing**: One of the main challenges with RNNs, including LSTMs, is the difficulty in parallelizing their training process. Since the computation at each time step depends on the previous time step, it is inherently sequential, making it challenging to take full advantage of parallel computing resources.\n",
    "\n",
    "5. **Lack of Interpretability**: Like other deep learning models, LSTMs can be difficult to interpret and understand. The internal representations and gating mechanisms can be hard to visualize or explain, making it challenging to diagnose errors or gain insights into the model's decision-making process.\n",
    "\n",
    "6. **Vanishing Gradients**: While LSTMs are designed to alleviate the vanishing gradient problem to some extent, they are not entirely immune to it. In some cases, especially when dealing with extremely long sequences, LSTMs can still suffer from vanishing gradients, making it challenging to learn very long-range dependencies.\n",
    "\n",
    "Despite these challenges, LSTMs have proven to be effective for a wide range of applications involving sequential data. Alternative architectures, such as Gated Recurrent Units (GRUs) and Transformers, have been proposed to address some of these issues and offer different trade-offs in terms of complexity, computational requirements, and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer phases ## \n",
    "\n",
    "The transformer model consists of two main phases: the training phase and the inference phase. These two phases have different objectives and processes, and serve different purposes in the overall functioning of the transformer network.\n",
    "\n",
    "During the training phase, the transformer model learns to predict the output sequence from the input sequence using a process called supervised learning. In this phase, the model is trained on a large dataset of input/output pairs, where the correct output for each input sequence is provided as a label.\n",
    "\n",
    "The training process involves adjusting the weights of the model based on the error between the predicted output and the true output for each input sequence. This is done using a technique called backpropagation, which involves computing the gradient of the error with respect to each weight in the network, and then using this gradient to update the weights in the direction that reduces the error.\n",
    "\n",
    "The goal of the training phase is to optimize the weights of the model so that it can accurately predict the output sequence for any input sequence it encounters. This requires the model to learn the underlying patterns and relationships between the input and output sequences, and to generalize these patterns to new input sequences that were not seen during training.\n",
    "\n",
    "During the inference phase, the transformer model is used to generate the output sequence for a new input sequence. In this phase, the model applies the learned weights and parameters to the input sequence to compute the output sequence.\n",
    "\n",
    "In the inference phase, the input sequence is first passed through the encoder component of the transformer, which generates the query, key, and value matrices for the self-attention mechanism. The decoder component of the transformer then uses these matrices to compute the output sequence, one token at a time.\n",
    "\n",
    "Unlike the training phase, the goal of the inference phase is to generate accurate output sequences for new input sequences that were not seen during training, rather than to learn the underlying patterns and relationships between the input and output sequences. This requires the model to apply the learned weights and parameters in a flexible and efficient way, in order to generate accurate output sequences for a wide range of input sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer components ### \n",
    "\n",
    "A Transformer is composed of several key components that work together to process sequential data, particularly in natural language processing tasks. The main components of the Transformer architecture are:\n",
    "\n",
    "1. **Input Embeddings**: The input tokens are converted into continuous vectors using an embedding layer. These embeddings represent the semantic meaning of the input tokens and serve as the initial input for the Transformer model.\n",
    "\n",
    "2. **Positional Encoding**: To provide the Transformer with positional information about the input sequence, positional encodings are added to the input embeddings. These encodings are designed to capture the relative positions of tokens in the sequence, allowing the model to consider the order of the elements.\n",
    "\n",
    "3. **Encoder**: The encoder is a stack of identical layers that process the input sequence and generate a continuous representation. Each layer in the encoder contains two main sub-layers: multi-head self-attention and position-wise feed-forward networks. The output of the encoder serves as the input for the decoder.\n",
    "\n",
    "   3.1. **Multi-Head Self-Attention**: This sub-layer computes the relationships between different elements in the input sequence. The attention mechanism is applied multiple times in parallel (multi-head) to learn different representations of the input sequence, capturing various aspects of the relationships between elements.\n",
    "\n",
    "   3.2. **Position-wise Feed-Forward Networks**: These networks consist of fully connected layers that are applied to each position separately and identically. They help learn non-linear relationships between the input and output representations.\n",
    "\n",
    "4.\n",
    "**Decoder**: The decoder is also a stack of identical layers, responsible for generating the output sequence based on the encoder's output and the previously generated elements. Each layer in the decoder contains three main sub-layers: multi-head self-attention, encoder-decoder attention, and position-wise feed-forward networks.\n",
    "\n",
    "   4.1. **Multi-Head Self-Attention**: Similar to the encoder, this sub-layer in the decoder computes the relationships between elements in the target sequence. It allows the decoder to focus on different parts of the target sequence while generating the output.\n",
    "\n",
    "   4.2. **Encoder-Decoder Attention**: This sub-layer allows the decoder to attend to the encoder's output, providing a mechanism for the decoder to focus on specific parts of the input sequence while generating the output.\n",
    "\n",
    "   4.3. **Position-wise Feed-Forward Networks**: Like in the encoder, these networks are applied to each position separately and identically, helping learn non-linear relationships between the input and output representations.\n",
    "\n",
    "5. **Residual Connections**: Residual connections are employed in both the encoder and decoder layers to facilitate the flow of gradients during backpropagation. Each sub-layer in the Transformer (e.g., multi-head self-attention or feed-forward neural network) has a residual connection followed by layer normalization.\n",
    "\n",
    "6. **Layer Normalization**: Layer normalization is used within the Transformer layers to stabilize training and improve generalization. It is applied before the activation function and helps alleviate the internal covariate shift problem, leading to faster training and better performance.\n",
    "\n",
    "7. **Output Linear Layer**: The final output of the decoder is passed through a linear layer that projects the hidden state dimensions back to the vocabulary size, providing a probability distribution over the target vocabulary.\n",
    "\n",
    "8. **Softmax and Loss Function**: The softmax function is applied to the output of the linear layer to convert the logits into probabilities. During training, a loss function (e.g., cross-entropy loss) is used to measure the difference between the predicted probabilities and the true target sequence, guiding the optimization process.\n",
    "9. \n",
    "**Optimization and Learning Rate Scheduling**: The Transformer model is trained using gradient-based optimization algorithms, such as Adam, to minimize the loss function. Learning rate scheduling techniques, such as the warm-up and cool-down strategy, are often used to adjust the learning rate during training. These strategies help the model converge faster and achieve better performance.\n",
    "\n",
    "10. **Dropout**: To regularize the model and prevent overfitting, dropout is applied to various components of the Transformer, such as the input embeddings, residual connections, and the output of the multi-head self-attention and feed-forward neural network layers. Dropout randomly drops a proportion of the connections during training, encouraging the model to learn more robust representations.\n",
    "\n",
    "Overall, the Transformer architecture combines these components to create a powerful model capable of processing sequential data effectively. It has become the foundation for many state-of-the-art models in natural language processing and other domains, such as BERT, GPT, and T5, due to its ability to capture long-range dependencies and scale to large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphics/transformer-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are input embeddings? ###\n",
    "\n",
    "Input embeddings in Transformers are dense vector representations of the input tokens, which are used as the input for the model. These embeddings capture semantic information about the tokens, allowing the model to process the input sequence effectively. In the case of natural language processing tasks, input embeddings are often initialized using pre-trained word embeddings, such as Word2Vec, GloVe, or BERT, and are fine-tuned during training.\n",
    "\n",
    "Here is a detailed explanation of the process of creating input embeddings for a Transformer:\n",
    "\n",
    "1. **Tokenization**: The input text is first tokenized into individual tokens (words, subwords, or characters, depending on the tokenization strategy). For example, consider the sentence \"The cat sat on the mat.\" Using word-level tokenization, we would obtain the following tokens: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"].\n",
    "\n",
    "2. **Token IDs**: Each token is then mapped to a unique identifier, usually an integer, based on a pre-defined vocabulary. The vocabulary is a mapping between tokens and their corresponding IDs. For our example, the tokens might be mapped to the following IDs: [1, 2, 3, 4, 1, 5].\n",
    "\n",
    "3. **Embedding lookup**: The token IDs are used to look up their corresponding embeddings in an embedding matrix. The embedding matrix is a learnable parameter of the model, with its dimensions being the vocabulary size (number of unique tokens) and the embedding size (dimensionality of the embeddings). In our example, if we have an embedding size of 5, the embedding matrix would have dimensions (6, 5), and the token IDs would be used to look up their corresponding 5-dimensional embeddings.\n",
    "\n",
    "4. **Positional encoding**: To provide the model with information about the position of each token in the input sequence, positional encodings are added to the input embeddings. Positional encodings can be either learned or fixed, with sinusoidal functions being a common choice for fixed positional encodings. The resulting embeddings, with positional information added, are used as the input to the Transformer.\n",
    "\n",
    "For our example sentence \"The cat sat on the mat\", after tokenization, mapping to token IDs, and looking up the embeddings, we would obtain a matrix of size (6, 5), where each row represents the 5-dimensional embedding of a token. Once the positional encoding is added, this matrix is fed into the Transformer model for processing.\n",
    "\n",
    "In summary, input embeddings in Transformers are dense vector representations of the input tokens, created by looking up the embeddings corresponding to the token IDs and adding positional encodings. These embeddings are used as the input for the model, allowing it to process and learn from the input sequence effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding matrix is a learnable parameter in a neural network that contains the dense vector representations of the tokens in the vocabulary. Each row in the embedding matrix corresponds to a token in the vocabulary, and the number of columns represents the dimensionality of the embeddings.\n",
    "\n",
    "To provide a better understanding, let's consider a simple example. Suppose you have a vocabulary of 10,000 unique tokens (words, subwords, or characters) and you've chosen an embedding size of 100. The embedding matrix will have dimensions of 10,000 x 100. Each row in the matrix represents the 100-dimensional vector for a specific token in the vocabulary.\n",
    "\n",
    "During the initial stages of training, the values in the embedding matrix are usually randomly initialized or initialized with pre-trained embeddings like Word2Vec or GloVe, if available. As the neural network is trained, the values in the embedding matrix are updated through backpropagation. This fine-tuning process allows the embeddings to capture the semantic relationships between tokens in the context of the specific task the model is trained for.\n",
    "\n",
    "When the model processes an input sequence, the token IDs are used to look up their corresponding embeddings in the embedding matrix. The resulting matrix of embeddings (with added positional encodings) is then fed into the model as input.\n",
    "\n",
    "In summary, the embedding matrix contains the dense vector representations of the tokens in the vocabulary. It is a learnable parameter in the model, with its dimensions determined by the vocabulary size and the chosen embedding size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings explained ###\n",
    "\n",
    "In the context of word embeddings, dimensionality refers to the number of dimensions or features in the continuous vector space where the words (or tokens) are embedded. Each dimension represents a latent semantic feature that helps capture the relationships between different words in the vocabulary.\n",
    "\n",
    "When we create word embeddings, we're converting words from a discrete, one-hot encoded representation into a continuous vector space. This continuous representation is more compact and efficient for a neural network to process.\n",
    "\n",
    "The dimensionality of the embedding space (or the embedding size) determines how many features are used to represent each word in this continuous space. A higher-dimensional embedding space can capture more information about the relationships between words but may also require more data to train effectively and be computationally more expensive.\n",
    "\n",
    "For example, consider you have an embedding size of `d`. When you convert a word from your vocabulary into a word embedding, you'll represent it as a continuous vector with `d` dimensions. Each value in this `d`-dimensional vector corresponds to a specific latent feature that contributes to the overall semantic representation of the word.\n",
    "\n",
    "In summary, the dimensionality of word embeddings refers to the number of features or dimensions in the continuous vector space where words are represented. These dimensions capture latent semantic features, enabling the neural network to learn complex patterns in the data more effectively than with discrete, one-hot encoded representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings explained like I'm a five year old. ###\n",
    "\n",
    "Imagine you have 6 toy blocks, and each block has a word from the sentence \"The cat sat on the mat.\" You want to organize these blocks in a way that similar words are close to each other, and different words are far apart. To do this, you can arrange the blocks on a big table.\n",
    "\n",
    "In this example, the table is like the \"embedding space\" for words, and the \"dimensionality\" is like the number of directions you can arrange the blocks on the table. If your table is flat, you can only arrange the blocks in two directions: left-right and up-down. But if you have a 3D table, you can also arrange the blocks in and out.\n",
    "\n",
    "Now, let's say you have a 2D table. You might arrange the words \"cat\" and \"mat\" close together because they both end in \"-at,\" and they are both things. The words \"sat\" and \"on\" might be placed closer together because they are both action words.\n",
    "\n",
    "The more directions (dimensions) you have, the better you can arrange the blocks so that similar words are close together, and different words are far apart. But if you have too many directions, it becomes harder to find the best way to arrange the blocks.\n",
    "\n",
    "So, dimensionality in word embeddings is like the number of directions you can arrange words from the sentence \"The cat sat on the mat\" in a special space, which helps computers understand the relationships between words better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are Transformers and the Attention model better than RNN's and LSTM's?\n",
    "\n",
    "The attention mechanism in Transformers offers several advantages over RNNs and LSTMs, making them better suited for processing sequential data in many tasks:\n",
    "\n",
    "1. **Parallelization**: Unlike RNNs and LSTMs, which process input sequences in a sequential manner, the attention mechanism in Transformers allows for parallel computation across all input positions. This leads to significantly faster training and inference times, especially on modern hardware like GPUs.\n",
    "\n",
    "2. **Long-range dependencies**: RNNs and LSTMs can struggle to capture long-range dependencies due to the vanishing gradient problem. In contrast, the attention mechanism in Transformers can directly model relationships between any pair of input positions, regardless of their distance, making it easier to capture long-range dependencies in the data.\n",
    "\n",
    "3. **Global context**: The attention mechanism in Transformers computes a weighted sum of all input positions for each output position, allowing the model to consider the global context of the input sequence when making predictions. This can lead to better performance in tasks that require understanding the entire sequence, such as translation or summarization.\n",
    "\n",
    "4. **Interpretability**: The attention mechanism produces attention weights that can be visualized and analyzed, providing insights into the model's decision-making process. This can help understand which parts of the input sequence are most relevant for a particular prediction, making the model more interpretable than RNNs and LSTMs.\n",
    "\n",
    "5. **Scalability**: Transformers have demonstrated excellent scalability to large datasets and long sequences. As the attention mechanism can be computed efficiently on modern hardware, Transformers can be easily scaled to handle larger input sequences or larger models with more layers and parameters, leading to improved performance in various tasks.\n",
    "\n",
    "These advantages have made the attention mechanism in Transformers a popular choice for sequential data processing tasks, often outperforming RNNs and LSTMs in terms of accuracy and efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is positional encoding ##\n",
    "Positional encoding is a technique used in Transformer models to provide information about the position of tokens in a sequence. Transformers don't have an inherent sense of the order of tokens like RNNs or LSTMs, as they process tokens in parallel. To address this, positional encoding is added to the input embeddings to ensure that the model can consider the order of the tokens while processing the input sequence.\n",
    "\n",
    "Positional encoding can be implemented in various ways, but one of the most common methods is using sinusoidal functions. The idea is to create a set of vectors that can be added to the input embeddings without changing their dimensionality. These vectors have a unique pattern that allows the model to easily learn and recognize the positions of the tokens in the sequence.\n",
    "\n",
    "In the sinusoidal positional encoding, each position (i) in the input sequence gets a vector of the same size as the input embeddings (d). The vector is created by applying sinusoidal functions with different frequencies to the position index:\n",
    "\n",
    "$PE(pos, 2i) = sin(pos / 10000^(2i/d))$\n",
    "$PE(pos, 2i+1) = cos(pos / 10000^(2i/d))$\n",
    "\n",
    "Here, pos is the position of the token in the sequence, and i ranges from 0 to d/2. This results in a unique positional encoding vector for each position in the input sequence.\n",
    "\n",
    "Once the positional encoding vectors are computed, they are added to the corresponding input embeddings. The combined embeddings, which now contain both the token information and the position information, are then fed into the Transformer model.\n",
    "\n",
    "In summary, positional encoding is necessary for Transformers because it provides information about the position of tokens in a sequence, which is crucial for understanding the context and relationships between tokens. Sinusoidal positional encoding is a popular method to achieve this, as it generates unique and easily distinguishable patterns for each position in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the positional encoding used by transformers, both sine and cosine functions are used together to encode the position of each token in the sequence.\n",
    "\n",
    "The positional encoding consists of a vector with a fixed size for each token, and the vector is made up of different combinations of sine and cosine functions with different frequencies.\n",
    "\n",
    "Specifically, for each position in the sequence, two values are generated - one using sine function and another using cosine function - and these values are added to the token's embedding vector. The value generated by the sine function is added to the odd-indexed elements of the embedding vector, while the value generated by the cosine function is added to the even-indexed elements of the embedding vector.\n",
    "\n",
    "So both sine and cosine functions are used to create a unique and distinguishable representation for each position in the sequence, which allows the transformer to understand the order and the relative distance between the tokens in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding explained like I'm a five year old ## \n",
    "\n",
    "Imagine you have a box of letter magnets, like the ones you put on your fridge. You can use these magnets to create sentences by putting the letters in a specific order. Now, let's say you have a robot friend who wants to help you understand the meaning of these sentences. \n",
    "\n",
    "The robot can only look at all the magnets at once, but it doesn't know the order in which you put the magnets. So, to help your robot friend, you put special stickers on each magnet that tell the robot where the magnet should be in the sentence. These stickers are like secret codes that show the position of the magnets in the sentence.\n",
    "\n",
    "Positional encoding is like those stickers. It helps the robot (the Transformer model) understand the order of the words in a sentence so that it can learn to make sense of the sentence and find out what it means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoders demystified ##\n",
    "The encoder in a Transformer processes the input sentence to understand its structure, meaning, and relationships between the words. Here's a detailed step-by-step explanation of what the encoder does:\n",
    "\n",
    "1. **Input Embeddings**: First, the input sentence is converted into a sequence of word embeddings. Each word is represented as a high-dimensional vector that captures its semantic meaning. These embeddings are then combined with positional encodings to give the model information about the position of each word in the sentence.\n",
    "\n",
    "2. **Multi-Head Self-Attention**: The encoder uses a self-attention mechanism to analyze the relationships between different words in the sentence. The self-attention mechanism computes an attention score for each pair of words, indicating the importance of one word to another in the context of the sentence. The multi-head self-attention mechanism repeats this process multiple times (with different sets of learned parameters) to capture various aspects of the relationships between the words.\n",
    "\n",
    "3. **Layer Normalization**: After the multi-head self-attention, the outputs are combined with the original input embeddings through a residual connection, and then a layer normalization is applied. Layer normalization helps in stabilizing the training process and mitigates the risk of exploding gradients.\n",
    "\n",
    "4. **Feed-Forward Neural Network**: Following the layer normalization, the output is passed through a position-wise feed-forward neural network. This is a simple neural network that is applied independently to each position (word) in the sentence, allowing the model to learn more complex representations of the input.\n",
    "\n",
    "5. **Output of the Encoder Layer**: The output of the feed-forward neural network is combined with the input through another residual connection, followed by layer normalization. This output is then sent to the next layer of the encoder (if there are more layers).\n",
    "\n",
    "The encoder consists of multiple layers, each performing the same steps mentioned above. The output from the final layer of the encoder is a transformed representation of the input sentence that captures its meaning, structure, and relationships between the words. This output is then passed to the decoder to generate the desired output, such as a translation in another language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Attention?  ##\n",
    "\n",
    "**Attention** is a mechanism in neural networks that helps the model focus on the important parts of the input data. In the context of natural language processing, attention enables the model to weigh the relevance of each word in the input sequence, depending on the task being performed. It helps the model to understand which words should be given more importance when generating the output.\n",
    "\n",
    "For example, imagine we're translating a sentence from one language to another. The attention mechanism allows the model to \"pay attention\" to specific words in the input sentence while generating a corresponding word in the output sentence. This helps the model to capture the dependencies between the input and output words, even when they are far apart in the sentence.\n",
    "\n",
    "**Multi-Head Self-Attention** is an extension of the attention mechanism used in Transformer models. It allows the model to focus on different aspects of the relationships between words in the input sentence simultaneously. In multi-head self-attention, the model computes multiple attention scores for each pair of words, using different sets of learned parameters (called \"heads\"). Each head can capture different aspects of the relationships between words, making the model more expressive and better able to understand the input sentence.\n",
    "\n",
    "Here's a simple example: Suppose we have a sentence, \"The cat sat on the mat.\" The multi-head self-attention mechanism can help the model understand various aspects of the relationships between the words, such as:\n",
    "\n",
    "- Head 1: Focuses on understanding the grammatical structure (e.g., \"cat\" is the subject, \"sat\" is the verb, and \"mat\" is the object)\n",
    "- Head 2: Focuses on capturing semantic meaning (e.g., \"cat\" and \"mat\" are related because cats often sit on mats)\n",
    "\n",
    "By using multiple attention heads, the Transformer can learn to understand the input sentence more comprehensively, enabling it to perform complex tasks such as translation, summarization, and question-answering more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a good analogy: \n",
    "\n",
    "**Attention** is like when you're listening to a story and you pay more attention to the most important words to understand the story better. In a computer, this helps it figure out which words are more important when it's working with sentences.\n",
    "\n",
    "**Multi-Head Self-Attention** is like having multiple \"listeners\" inside the computer, and each listener pays attention to different parts of the story at the same time. This way, the computer can understand the story even better. For example, one listener might focus on who's in the story, while another might focus on what's happening.\n",
    "\n",
    "So, if the story is, \"The cat sat on the mat,\" one listener might focus on understanding that it's about a cat and a mat, while another listener might focus on the action of sitting.\n",
    "\n",
    "Each head doesn't know exactly what to listen for from the start. Instead, they learn to pay attention to different things as they train on lots of sentences. They automatically figure out what's important by looking for patterns and relationships between words in the sentences.\n",
    "\n",
    "For example, at the beginning, all the heads might pay attention to everything equally. But as they see more sentences, they start to learn what's important. One head might notice that focusing on the subject (like the cat) helps understand the story better, while another head might find that focusing on the action (like sitting) is more useful.\n",
    "\n",
    "So, over time, each head learns to listen for different things that help it understand the sentences better. And when all the heads work together, the computer can understand the whole story really well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the specific steps that a head in the multi-head self-attention mechanism performs:\n",
    "\n",
    "1. **Linear Projections**: Each head gets the input words (as embeddings) and then creates three different sets of vectors from them: query, key, and value vectors. These vectors are created by multiplying the input embeddings with separate weight matrices, which are learned during training.\n",
    "\n",
    "2. **Calculate Attention Scores**: The head computes attention scores for every word in the sentence. It does this by taking the dot product between the query vector of a word and the key vectors of all other words in the sentence. This gives a measure of how much a word should pay attention to other words.\n",
    "\n",
    "3. **Apply Softmax**: The attention scores are then passed through a softmax function to convert them into probabilities. This helps the head focus more on higher-scoring words and less on lower-scoring words.\n",
    "\n",
    "4. **Calculate Output Vectors**: The head multiplies the softmax probabilities with the corresponding value vectors. This step gives us a weighted sum of the value vectors, where the weights are determined by the attention probabilities. In other words, the more attention a word gets, the more it contributes to the output vector.\n",
    "\n",
    "5. **Combine Output Vectors**: After all the heads have completed these steps, their output vectors are concatenated and then transformed using another learned linear projection. This final output represents a combination of information from all the heads, which can be passed on to the next layer in the Transformer.\n",
    "\n",
    "By following these steps, each head can focus on different aspects of the input sentence and learn different relationships between the words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the attention mechanism, particularly in the context of Transformers, the query, key, and value tensors play a crucial role in determining the relationship between different input tokens and their corresponding output representations. Let's break down their roles and how they are calculated using a concrete example.\n",
    "\n",
    "Assume we have the following input sentence: \"The cat sat on the mat.\"\n",
    "\n",
    "1. **Tokenization**: First, the sentence is tokenized into words (tokens): [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "2. **Embedding**: Each token is converted into a continuous vector representation (embedding) using an embedding matrix. Let's assume the embedding dimension is `d_model`.\n",
    "\n",
    "3. **Linear Projections**: For the multi-head self-attention mechanism, we create three different sets of linear projections (weight matrices) for each token's embedding - one for the queries$(W_q)$, one for the keys $(W_k)$, and one for the values ($W_v$). These weight matrices have dimensions $(d_model, d_k)$ for $W_q$ and W_k, and $(d_model, d_v)$ for $W_v$, where $d_k$ and $d_v$ are the dimensions of the key and value vectors, respectively.\n",
    "\n",
    "4. **Calculate Query, Key, and Value Vectors**: We calculate the query, key, and value vectors for each token by multiplying the token's embedding with the corresponding weight matrices $W_q$, $W_k$, and $W_v$:\n",
    "\n",
    "   - Query: Q = token_embedding * $W_q$\n",
    "   - Key: K = token_embedding * $W_k$\n",
    "   - Value: V = token_embedding * $W_v$\n",
    "\n",
    "Now, let's assume we are calculating the attention for the token \"cat\". The query vector for \"cat\" represents its role in the current context when attending to other tokens, the key vectors for other tokens represent how relevant they are to the \"cat\" token, and the value vectors hold the information from each token that will be aggregated in the attention output.\n",
    "\n",
    "5. **Calculate Attention Scores**: To determine the attention scores, we perform a dot product between the query vector of \"cat\" and the key vectors of all other tokens, including itself. This will result in a scalar value representing the compatibility between \"cat\" and each token. Higher scores indicate a stronger relationship.\n",
    "\n",
    "6. **Apply Softmax**: To convert the attention scores into probabilities, we apply the softmax function across the scores. The softmax-normalized scores represent the attention distribution for the \"cat\" token over all the tokens in the sequence.\n",
    "\n",
    "7. **Calculate Attention Output**: Finally, we compute the attention output for the \"cat\" token by taking the weighted sum of the value vectors, using the softmax-normalized attention scores as weights. This results in a new vector representation for the \"cat\" token, which is an aggregation of information from all the tokens in the sequence, with a higher focus on the tokens that have a stronger relationship with \"cat\".\n",
    "\n",
    "These steps are performed for every token in the sequence, and the process is repeated for each attention head, which allows the model to focus on different aspects of the relationships between tokens. The outputs from all the attention heads are then concatenated and passed through a linear layer to produce the final output for each token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"graphics/transformer_attention_heads_qkv.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A concrete example ##\n",
    "\n",
    "\n",
    "Embedding the input: The input \"The cat sat on the mat\" is first embedded using an embedding matrix to obtain a sequence of embedding vectors, where each vector represents a word in the input.\n",
    "\n",
    "Generating the query, key, and value matrices: The query, key, and value matrices are generated from the embedding vectors. For example, suppose we use a transformer model with 3 attention heads, and an embedding dimension of 4. Then the query, key, and value matrices would each contain 6 vectors (one for each word in the input), with each vector having a length of 4.\n",
    "\n",
    "Calculating the dot product: For each attention head, the dot product is computed between the query and key vectors in the corresponding positions of the query and key matrices. The dot product is computed by taking the element-wise product of the query and key vectors, and then summing the resulting products. The result is a set of attention weights for each word in the input.\n",
    "\n",
    "Scaling and normalization: The dot product values are scaled by the square root of the embedding dimension (in this case, the square root of 4 is 2) to avoid the values becoming too large or too small. The scaled dot product values are then passed through a softmax function to normalize them and obtain the attention weights.\n",
    "\n",
    "Calculating the context vector: The attention weights are used to compute a weighted sum of the value vectors in the value matrix. The resulting vector is the context vector for the input sequence, which captures the most relevant information in the sequence based on the attention weights.\n",
    "\n",
    "For example, suppose we want to compute the context vector for the word \"cat\" in the input sequence \"The cat sat on the mat\". The corresponding query vector would be [0, 0, 1, 0], indicating that we are interested in the word \"cat\" in the input sequence. The key matrix would contain the embeddings of all the words in the input sequence, while the value matrix would contain the importance or \"value\" of each word.\n",
    "\n",
    "The dot product between the query vector and each key vector in the key matrix would be computed for each attention head, resulting in three sets of dot products. These dot products are then scaled and passed through a softmax function to obtain the attention weights.\n",
    "\n",
    "Finally, the attention weights are used to compute a weighted sum of the value vectors in the value matrix, where the weights correspond to the attention weights. This produces the context vector, which represents the importance of each position in the input sequence, based on the similarity between the query vector and the key vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again with the five year old explanations ##\n",
    "Imagine you and your friends are sitting in a circle, and you're playing a game where each of you has a secret word. Your goal is to find out which friends' words are most related to your secret word.\n",
    "\n",
    "1. You and your friends turn your secret words into special codes (embeddings).\n",
    "2. You create three sets of stickers (linear projections) for each secret word: one for questions (query), one for keys (important features), and one for treasure boxes (value).\n",
    "3. You stick the question sticker on yourself, the key stickers on your friends, and the treasure box stickers on everyone, including yourself.\n",
    "4. You compare your question sticker with all the key stickers to see how well they match (attention scores).\n",
    "5. You give points (softmax) to each friend based on how well their key sticker matches your question sticker.\n",
    "6. Finally, you collect all the treasure boxes and put them together based on the points you gave your friends (attention output).\n",
    "\n",
    "You do this for every secret word, and you play the game with different stickers (multi-head attention) to find more connections between your words. In the end, you have a big treasure box filled with information about how all the secret words relate to each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the encoder processes the input and generates its output, the purpose of sending this output to a feed-forward neural network (FFNN) is to:\n",
    "\n",
    "1. Enhance the representation: The FFNN can capture more complex relationships and patterns within the encoder's output. It helps to refine and enhance the information obtained from the attention mechanism.\n",
    "\n",
    "2. Learn non-linear relationships: As the FFNN contains non-linear activation functions, it can learn non-linear relationships between the input tokens. This adds another level of complexity and expressiveness to the model.\n",
    "\n",
    "3. Maintain the sequence length: The FFNN operates independently on each position in the input sequence. As a result, it maintains the same sequence length throughout the processing, which is important for maintaining the input-output correspondence.\n",
    "\n",
    "The FFNN acts as an additional processing step to improve the quality of the representations generated by the encoder, making it more effective in capturing complex relationships and patterns in the input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is the query matrix calculated? ##\n",
    "\n",
    "In self-attention, the query matrix is derived from the embedded input sequence. Specifically, the embedded input sequence is passed through three learned linear transformations - one for generating the query matrix, one for the key matrix, and one for the value matrix.\n",
    "\n",
    "The embedded input sequence can be viewed as a matrix with shape (sequence length, embedding dimension), where each row represents the embedding of a token in the sequence. The linear transformations are applied to this matrix to generate the query, key, and value matrices with the same shape.\n",
    "\n",
    "The query matrix represents the set of queries that the model uses to attend over the key-value pairs. Each query vector is a linear combination of the embedding vectors of all tokens in the sequence, and is used to compute the attention weights for the corresponding token.\n",
    "\n",
    "The key matrix represents the set of keys that the model uses to calculate the dot product with each query vector. Each key vector is also a linear combination of the embedding vectors of all tokens in the sequence.\n",
    "\n",
    "The value matrix represents the set of values that the model uses to weight the key-value pairs. Each value vector is a linear combination of the embedding vectors of all tokens in the sequence, and is used to compute the context vector that summarizes the important information in the input sequence.\n",
    "\n",
    "So, in summary, the query matrix is derived from the embedded input sequence through a learned linear transformation. Each query vector in the matrix represents a weighted combination of the embedding vectors of all tokens in the sequence, and is used to compute the attention weights for the corresponding token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is the key matrix calculated? ## \n",
    "\n",
    "\n",
    "The  matrix that transforms each word into a set of numbers called a key vector.\n",
    "\n",
    "The key matrix is like a table that stores all of the key vectors for the words in the sentence. Each row of the table corresponds to a word in the sentence, and the numbers in the row represent the key vector for that word. These key vectors are used to calculate the importance of each word in the sentence.\n",
    "\n",
    "To create the key matrix, we first take the embedded input sequence, which is a matrix that contains the numerical representations of the words in the sentence. We then multiply this matrix by the learned weight matrix to transform each word into a key vector. This key vector is like a unique code that represents the meaning of that word.\n",
    "\n",
    "Once we have the key matrix, we can use it to calculate the importance of each word in the sentence. We do this by comparing each word's key vector to a \"query\" vector, which is like a question that we ask about the sentence. We use the dot product to compare the query vector with each key vector in the key matrix, and then use the results to calculate how important each word is.\n",
    "\n",
    "So, in summary, the key matrix is a table that stores the key vectors for the words in a sentence. The key vectors are created by multiplying the embedded input sequence by a learned weight matrix, and represent the meaning of each word. The key matrix is used to calculate the importance of each word in the sentence by comparing the key vectors to a query vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is the value matrix calculated ## \n",
    "In a self-attention mechanism, the value matrix is also calculated from the embedded input sequence by applying a learned linear transformation to it, similar to how the key matrix is calculated.\n",
    "\n",
    "The embedded input sequence can be represented as a matrix with shape (sequence length, embedding dimension), where each row represents the embedding of a token in the sequence. To compute the value matrix, this matrix is multiplied by a learned weight matrix that transforms each embedding vector into a value vector of the same dimension.\n",
    "\n",
    "More specifically, the value matrix is computed as follows:\n",
    "\n",
    "    Take the embedded input sequence, represented as a matrix with shape (sequence length, embedding dimension).\n",
    "    Multiply this matrix by a learned weight matrix that transforms each embedding vector into a value vector of the same dimension. This transformation is usually a simple linear transformation that consists of a matrix multiplication followed by a bias term.\n",
    "    The resulting matrix is the value matrix, with shape (sequence length, embedding dimension).\n",
    "\n",
    "The value matrix contains a set of value vectors, where each value vector represents the embedding of a token in the input sequence. These value vectors are used to compute the context vector that summarizes the important information in the input sequence.\n",
    "\n",
    "To compute the context vector, the attention weights computed from the key and query matrices are applied to the value matrix. Specifically, each attention weight is applied to the corresponding value vector, and the resulting weighted value vectors are added together to produce the context vector. This context vector represents a summary of the important information in the input sequence, based on the attention weights.\n",
    "\n",
    "So, in summary, the value matrix is computed by applying a learned linear transformation to the embedded input sequence, which produces a set of value vectors that represent the embedding of each token in the sequence. These value vectors are then used to compute the context vector that summarizes the important information in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the input sequence \"Jane saw the dog run. The dog ran quickly.\" We can calulate the query matrix \n",
    "Here is the step-by-step process:\n",
    "\n",
    "Tokenization: The input sequence is first tokenized into individual words, resulting in a list of tokens: [\"Jane\", \"saw\", \"the\", \"dog\", \"run\", \".\", \"The\", \"dog\", \"ran\", \"quickly\", \".\"]\n",
    "\n",
    "Embedding: Each token is then embedded using a pre-trained embedding model to obtain a sequence of embedding vectors, where each vector has a length of 4 in this example.\n",
    "\n",
    "Query Matrix: To calculate the query matrix, we use a linear transformation to project each embedding vector into a vector of dimension 4. This projection is performed by multiplying the embedding vectors with a learned query weight matrix, resulting in a sequence of query vectors, where each vector has a length of 4.\n",
    "\n",
    "Here is the resulting query matrix:\n",
    "[[ 1.6364,  0.0028, -1.0251, -0.1829],\n",
    " [ 0.3307, -0.1861,  0.3043, -0.1412],\n",
    " [ 0.2713, -0.5517,  0.5733, -0.3958],\n",
    " [ 1.2139,  0.0889, -0.4958,  0.1122],\n",
    " [-0.1583, -0.4669, -0.6927, -0.4977],\n",
    " [-0.7586, -1.3015,  0.6627, -0.2671],\n",
    " [ 1.5012,  0.1813, -0.3308,  0.1469],\n",
    " [ 1.2139,  0.0889, -0.4958,  0.1122],\n",
    " [ 0.7127, -0.3101, -0.4104, -0.2224],\n",
    " [-0.3187, -1.1829, -0.2763, -0.5746]]\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, each row of the query matrix corresponds to one token in the input sequence, where each column represents a feature of the embedding vectors. The values in the query matrix are learned during training and represent the relationship between different tokens in the input sequence. The dot product and softmax calculations are then performed to obtain the attention weights, which determine how much each token in the sequence contributes to the final output of the transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Tokenization: The input sequence is first tokenized into individual words, resulting in a list of tokens: [\"Jane\", \"saw\", \"the\", \"dog\", \"run\", \".\", \"The\", \"dog\", \"ran\", \"quickly\", \".\"]\n",
    "\n",
    "Embedding: Each token is then embedded using a pre-trained embedding model to obtain a sequence of embedding vectors, where each vector has a length of 4 in this example.\n",
    "\n",
    "Key Matrix: To calculate the key matrix, we use a linear transformation to project each embedding vector into a vector of dimension 4. This projection is performed by multiplying the embedding vectors with a learned key weight matrix, resulting in a sequence of key vectors, where each vector has a length of 4.\n",
    "\n",
    "Here is the resulting key matrix:\n",
    "[[ 1.6364,  0.0028, -1.0251, -0.1829],\n",
    " [ 0.3307, -0.1861,  0.3043, -0.1412],\n",
    " [ 0.2713, -0.5517,  0.5733, -0.3958],\n",
    " [ 1.2139,  0.0889, -0.4958,  0.1122],\n",
    " [-0.1583, -0.4669, -0.6927, -0.4977],\n",
    " [-0.7586, -1.3015,  0.6627, -0.2671],\n",
    " [ 1.5012,  0.1813, -0.3308,  0.1469],\n",
    " [ 1.2139,  0.0889, -0.4958,  0.1122],\n",
    " [ 0.7127, -0.3101, -0.4104, -0.2224],\n",
    " [-0.3187, -1.1829, -0.2763, -0.5746]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will notice that the values for the query and the key are identical.  This is because the self-attention mechanismm is only working on tne same input text.  Let's however, now do another query.  \"What animal did Jane see?\"  \n",
    "\n",
    "Here is the query matrix for this query. \n",
    "\n",
    "[[ 0.6437, -0.4646, -0.1418, -0.1326],\n",
    " [-1.3257,  0.0175, -0.3731, -0.2615],\n",
    " [-0.4165, -0.4745,  1.3916, -1.0233],\n",
    " [-0.2356, -0.1224,  0.0785, -0.2445],\n",
    " [-0.5809,  0.4748,  0.4855, -0.2357],\n",
    " [ 0.8021, -0.0794, -0.6998, -0.2306]]\n",
    "\n",
    "\n",
    "Note that the elements are very different . \n",
    "\n",
    "Here is the value matrix for this new query. \n",
    "[[-0.2282, 0.4508, 0.4243, -0.6823],\n",
    "[-0.1413, 0.2487, 0.2368, -0.3336],\n",
    "[ 0.3123, -0.0429, -0.1741, -0.1578],\n",
    "[ 0.2536, 0.0871, -0.0665, 0.2270],\n",
    "[ 0.6845, -0.2185, -0.1145, -0.4764],\n",
    "[ 0.3034, -0.0643, -0.1001, 0.0945],\n",
    "[-0.0025, -0.1595, 0.0467, -0.0862],\n",
    "[ 0.2536, 0.0871, -0.0665, 0.2270],\n",
    "[-0.0494, -0.0899, -0.0813, 0.5233],\n",
    "[-0.4556, -0.1349, 0.2709, -0.3602]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connections ##\n",
    "\n",
    "In a transformer encoder, residual connections are a way of allowing the model to retain information from previous layers during training.\n",
    "\n",
    "To understand why residual connections are necessary, it's important to note that a transformer encoder is composed of multiple layers, each of which performs a specific function, such as self-attention or feedforward transformation. The output of each layer is then passed to the next layer as input.\n",
    "\n",
    "During training, the model updates the weights of each layer in an attempt to minimize the loss function. However, as the number of layers increases, the gradient signal that is propagated back through the layers can become weaker and weaker. This is known as the \"vanishing gradient\" problem and can make it difficult for the model to learn complex relationships between tokens in the input sequence.\n",
    "\n",
    "Residual connections address this problem by allowing the model to bypass some layers and retain information from previous layers. Specifically, the output of each layer is added to the input of that layer (after being transformed by a learned linear projection) to create a residual connection. This means that the model can learn to adjust the output of each layer while retaining information from previous layers.\n",
    "\n",
    "In other words, residual connections allow the model to \"remember\" important information from previous layers, which can help it to better capture the complex relationships between tokens in the input sequence.\n",
    "\n",
    "So, in summary, residual connections are a way of allowing a transformer encoder to retain important information from previous layers during training. By adding the output of each layer to the input of that layer, the model can adjust the output of each layer while retaining information from previous layers, which can help it to better learn the complex relationships between tokens in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain it like I'm ten years old ##\n",
    "\n",
    "In a transformer encoder, there are lots of layers that help the model learn about the input sequence. But sometimes, the information can get lost as it passes through all of the layers. Residual connections are like a way to make sure that the model doesn't forget important information as it learns.\n",
    "\n",
    "Imagine you're building a tower out of blocks. Each block is like a layer in the transformer encoder. If you just stack the blocks on top of each other, the tower might get wobbly and fall over. But if you use some glue to hold each block in place, the tower will be more stable.\n",
    "\n",
    "In the transformer encoder, residual connections are like the glue that holds each layer in place. They help the model to remember important information from earlier layers, so that it can better understand the input sequence. This makes the model more stable and accurate, like a tower that doesn't wobble and fall over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Softmax? ##\n",
    "\n",
    "Softmax is an activation function that is commonly used in machine learning and deep learning, especially in classification tasks. It takes a vector of real numbers and transforms it into a probability distribution, where each element in the output vector represents the probability of the corresponding input element. The sum of all the probabilities in the output vector will always equal 1.\n",
    "\n",
    "In Transformers, the softmax function is used in the attention mechanism to convert attention scores into probabilities. Here's why it's important:\n",
    "\n",
    "1. **Normalization**: By applying the softmax function to the attention scores, we normalize the scores so that they lie between 0 and 1. This ensures that the scores can be interpreted as probabilities, and that they are more stable and easier to work with.\n",
    "\n",
    "2. **Focus**: The softmax function emphasizes larger attention scores by assigning them higher probabilities, while suppressing smaller scores with lower probabilities. This helps the model focus more on the important words in the input sequence, and less on the irrelevant ones.\n",
    "\n",
    "3. **Differentiability**: The softmax function is smooth and differentiable, which is crucial for gradient-based optimization methods like backpropagation. This allows the model to learn the best attention scores during training.\n",
    "\n",
    "In summary, the softmax function is used in Transformers to convert attention scores into probabilities, which are then used to weigh the importance of different words in the input sequence. This helps the model focus on relevant information and learn meaningful relationships between the words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical formula for the softmax function is as follows:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^N e^{x_j}}\n",
    "$$\n",
    "\n",
    "Here, $x_i$ is the input element at position $i$, and $N$ is the total number of elements in the input vector. The softmax function takes the exponential of each input element $x_i$ and divides it by the sum of the exponentials of all the input elements. This results in a probability distribution over the input elements, where each element's probability lies between 0 and 1, and the sum of all probabilities equals 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's recap what we've learned so far ##\n",
    "To calculate the output vectors in a Transformer, several steps are performed after the input has been processed by the encoders. Here's a step-by-step description:\n",
    "\n",
    "1. **Input Embedding**: Tokenize the input sequence and convert the tokens into input embeddings using the embedding matrix. Add positional encodings to these input embeddings to retain positional information.\n",
    "\n",
    "2. **Encoder Layers**: Pass the input embeddings through multiple encoder layers. Each encoder layer consists of a multi-head self-attention mechanism followed by a feed-forward neural network (FFNN).\n",
    "\n",
    "3. **Attention**: In the multi-head self-attention mechanism, the input embeddings are transformed into query, key, and value vectors. These vectors are used to calculate attention scores, which represent the relevance of each token with respect to others in the sequence. The attention scores are normalized using the softmax function. The value vectors are then weighted by these normalized attention scores and summed up to create the output of the attention mechanism.\n",
    "\n",
    "4. **Multi-head Self-Attention**: The attention process is repeated multiple times in parallel within each encoder layer, resulting in multiple output vectors called \"heads\". These heads are then concatenated and linearly transformed to form a single output vector for each position in the sequence.\n",
    "\n",
    "5. **Feed-Forward Neural Network**: The output of the multi-head self-attention mechanism is then passed through a feed-forward neural network, which refines the representations further and captures non-linear relationships in the data.\n",
    "\n",
    "6. **Normalization and Residual Connections**: After each attention and FFNN step, layer normalization and residual connections are applied to stabilize the learning process and help with gradient flow during backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoders unmystified ## \n",
    "\n",
    "The purpose of the decoder in a Transformer architecture is to generate target sequences, often in sequence-to-sequence tasks like machine translation, text summarization, or image captioning. The decoder takes the output from the encoder and the target sequence (shifted by one position) as inputs and generates the target sequence token by token. Here's a detailed description of what the decoder does:\n",
    "\n",
    "1. **Target Input Embedding**: Tokenize the target sequence (shifted by one position) and convert the tokens into target input embeddings using the embedding matrix. Add positional encodings to these input embeddings to retain positional information.\n",
    "\n",
    "2. **Masked Multi-head Self-Attention**: The target input embeddings are first passed through a masked multi-head self-attention mechanism. This mechanism is similar to the one used in the encoder, but with an additional mask to prevent the model from attending to future tokens in the target sequence, ensuring that the model generates the output one token at a time.\n",
    "\n",
    "3. **Encoder-Decoder Attention**: The output from the masked multi-head self-attention is then passed through an encoder-decoder attention mechanism. This step allows the decoder to focus on relevant parts of the input sequence when generating the target sequence. In this attention mechanism, the query vectors come from the decoder's masked self-attention output, while the key and value vectors come from the final output of the encoder layers.\n",
    "\n",
    "4. **Feed-Forward Neural Network**: After the encoder-decoder attention, the output vectors are passed through a feed-forward neural network, similar to the one used in the encoder layers. This step refines the representations further and captures non-linear relationships between the input and output sequences.\n",
    "\n",
    "5. **Normalization and Residual Connections**: As in the encoder layers, layer normalization and residual connections are applied after each attention and FFNN step to stabilize the learning process and help with gradient flow during backpropagation.\n",
    "\n",
    "6. **Decoder Layers**: Steps 2-5 are repeated for a predefined number of decoder layers, allowing the model to learn increasingly complex relationships between the input and output sequences.\n",
    "\n",
    "7. **Output Projection**: Once the output vectors have been processed by all the decoder layers, they are projected back to the vocabulary size using a linear transformation (output embedding matrix), followed by a softmax activation to produce probability distributions over the target vocabulary.\n",
    "\n",
    "8. **Prediction**: The token with the highest probability in the output distribution is selected as the predicted output at each position in the target sequence.\n",
    "\n",
    "The decoder, by processing the target input sequence and attending to the relevant parts of the encoded input sequence, generates the final output sequence in a step-by-step manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Input Embedding\n",
    "\n",
    "The decoder in a Transformer architecture is responsible for generating the output sequence based on the input sequence and the intermediate representations produced by the encoder. To achieve this, the decoder processes the target input sequence in a way similar to how the encoder processes the input sequence. The first step in this process is the target input embedding.\n",
    "\n",
    "The target input sequence is a sequence of tokens representing the desired output. In the case of language translation, for example, the target input sequence would be a sequence of tokens in the target language. The target input tokens are first converted into embeddings, just like the input tokens in the encoder.\n",
    "\n",
    "The target input embedding process is similar to the input embedding process in the encoder:\n",
    "\n",
    "1. Convert target input tokens into integer indices using a vocabulary.\n",
    "2. Map each integer index to an embedding vector using an embedding matrix.\n",
    "\n",
    "The resulting target input embeddings are then combined with positional encodings to incorporate information about the position of each token within the sequence. This is essential because the Transformer architecture doesn't have any inherent notion of the order of tokens in a sequence.\n",
    "\n",
    "Once the target input embeddings and positional encodings are combined, they are fed into the subsequent layers of the decoder. The decoder then processes this information, along with the output from the encoder, to generate the final output sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of the decoder in action ##\n",
    "Imagine you want to tell a story in another language. The decoder in a Transformer helps you do that. It takes words from the story and tries to guess what comes next in the other language. To do this, it looks at the words it already knows and also gets some help from the encoder part.\n",
    "\n",
    "The decoder starts by trying to guess the first word in the other language. It needs to know what word it's trying to guess, so it gets a little clue called the \"target input.\" This clue is like a hint to help it guess the word.\n",
    "\n",
    "To understand the clue, the decoder needs to turn it into something it can work with. So it changes the clue into a special code called an \"embedding.\" This code helps the decoder understand the clue better and remember what it's seen before.\n",
    "\n",
    "Now the decoder is ready to use the clue and start guessing the word. It works hard to make a good guess and then moves on to the next word in the story. It keeps doing this until it has guessed all the words in the other language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked  Multihead attention \n",
    "\n",
    "Masked Multi-head Attention is a variation of the Multi-head Attention mechanism used specifically in the Transformer's decoder. The main difference between the standard Multi-head Attention and the Masked Multi-head Attention is that the latter applies a \"mask\" to prevent the decoder from attending to future positions in the input sequence. This is important because, during training, the decoder should not have access to future information that it's trying to predict.\n",
    "\n",
    "The mask is typically a matrix that has the same size as the attention scores, with `1` at positions that correspond to illegal connections (future positions) and `0` at positions that correspond to legal connections (current and past positions). This mask is then added to the attention scores before applying the softmax function. The masked positions will have very low scores, making their impact on the final attention values negligible.\n",
    "\n",
    "Masked Multi-head Attention is used in the self-attention layer of the Transformer's decoder, where the input is the target sequence shifted by one position. By applying the mask, the decoder is forced to focus only on the words it has generated so far, thus preventing it from \"cheating\" by looking at future words in the target sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder should not have access to future positions in the input sequence because its primary goal is to generate the output sequence one token at a time, based on the information it has seen so far. In real-world applications, such as translation or text summarization, the model won't have access to the entire target sequence ahead of time. \n",
    "\n",
    "During training, if the decoder were allowed to see future tokens, it would be able to \"cheat\" by simply copying the correct answer from the future input. This would lead to a model that performs well during training but fails to generate meaningful outputs when deployed in real-world scenarios, where future tokens are not available.\n",
    "\n",
    "By masking the future positions in the input sequence, the decoder is forced to learn meaningful representations and context from the encoder's output and the target tokens it has generated so far. This way, the model learns to generate the output sequence step by step, making it more effective when deployed in real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have a magic machine that helps you translate a sentence from one language to another, like from English to French. Let's use the sentence \"I love apples\" as an example.\n",
    "\n",
    "The magic machine has two parts: the first part (encoder) reads and understands the English sentence, and the second part (decoder) creates the French sentence.\n",
    "\n",
    "The decoder works like this:\n",
    "\n",
    "1. The decoder starts with a special sign that tells it to begin translating.\n",
    "2. The decoder looks at the first word it has to create and makes sure it doesn't peek at the other words it needs to create later.\n",
    "3. The decoder also looks at what the first part of the machine (encoder) learned from the English sentence to make the French sentence make sense.\n",
    "4. The decoder then picks the best French word to start the sentence, like \"J'\".\n",
    "5. The decoder keeps doing this, one word at a time, until it finishes the whole French sentence.\n",
    "\n",
    "In the end, the magic machine turns \"I love apples\" into \"J'aime les pommes\"!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The OpenAI API ##\n",
    "OpenAI has made an API for all of its products available to developers.  Here are some examples of programs written using Python that use the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Set up API credentials\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Set up API credentials\n",
    "with open(\"~/key.txt\") as key:\n",
    "    openai.api_key = key.readline().strip()\n",
    "\n",
    "# Set up prompt\n",
    "prompt = \"Write a short story about a robot that learns to love.\"\n",
    "\n",
    "# Generate text using the GPT-3 language model\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-002\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=100,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "print(response.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bbrelin/src/repos/transformers/docs'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder starts outputting the output as tokens, beginnin with the <start> token and ending with the <end> token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a feed forward neural network ##\n",
    "A feed-forward neural network (FFNN) is a type of artificial neural network where information flows in one direction, from input to output, without any cycles or loops. It's one of the simplest and most widely used architectures in machine learning. Here's a detailed description of a feed-forward neural network:\n",
    "\n",
    "1. **Input Layer**: The input layer consists of nodes (neurons) that receive the input features. Each node in the input layer corresponds to one input feature. The values of the input features are passed to the next layer through weighted connections.\n",
    "\n",
    "2. **Hidden Layers**: A feed-forward neural network can have one or more hidden layers. Each hidden layer is composed of nodes, with each node representing a learned representation of the input features. These nodes apply an activation function to the weighted sum of their inputs, transforming the data non-linearly. The activation function can be ReLU, sigmoid, tanh, or any other non-linear function.\n",
    "\n",
    "3. **Output Layer**: The output layer consists of nodes that produce the final output of the network. The number of nodes in the output layer depends on the task. For example, in a classification task, the output layer would have as many nodes as there are classes. In a regression task, there would typically be a single output node. Like the hidden layer nodes, output layer nodes also use an activation function, which is chosen based on the specific problem being solved.\n",
    "\n",
    "4. **Weights and Biases**: The connections between nodes in adjacent layers have associated weights and biases. These are the learnable parameters of the neural network. During training, the weights and biases are adjusted to minimize the difference between the network's predicted outputs and the true outputs for the training data.\n",
    "\n",
    "5. **Forward Propagation**: When data is fed into the network, it goes through a process called forward propagation. The input values are passed through the input layer, then multiplied by the weights and summed up, with biases added in each node of the hidden layers. The activation function is applied to these sums, and the resulting values are passed to the next layer. This process continues until the output layer is reached, and the final output of the network is produced.\n",
    "\n",
    "6. **Backpropagation**: During training, the network's predictions are compared to the true outputs, and the error is calculated using a loss function. The backpropagation algorithm is then used to update the weights and biases in the network, minimizing the error. The algorithm computes the gradients of the loss function with respect to the weights and biases, starting from the output layer and moving backward through the network. These gradients are used to update the weights and biases using an optimization algorithm such as stochastic gradient descent (SGD) or one of its variants.\n",
    "\n",
    "In summary, a feed-forward neural network is a type of artificial neural network that processes information in a single direction, without any cycles or loops. It consists of an input layer, one or more hidden layers, and an output layer. During training, the network learns the optimal weights and biases to minimize the difference between its predictions and the true outputs.\n",
    "\n",
    "\n",
    "This is a graphical representation of a feed forward neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"graphics/neural-net.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-End Learning: A Technical Overview\n",
    "\n",
    "End-to-end learning is a training approach in machine learning, particularly in deep learning, where a single model learns to map raw inputs directly to desired outputs without relying on intermediate representations or hand-engineered features. The end-to-end learning paradigm simplifies the overall learning pipeline by allowing the model to learn the entire task from data, as opposed to relying on a series of separate modules or pre-processing steps.\n",
    "\n",
    "In traditional machine learning, the learning pipeline often consists of multiple stages:\n",
    "\n",
    "1. **Pre-processing**: Raw data is cleaned, transformed, and converted into a suitable format for machine learning algorithms. This stage may involve steps like normalization, scaling, or data augmentation.\n",
    "2. **Feature extraction**: Domain-specific knowledge and expertise are used to extract relevant features from the pre-processed data. These features are then used as input to the machine learning model.\n",
    "3. **Model training**: A machine learning model is trained using the extracted features and corresponding labels to learn a mapping between the features and the desired output.\n",
    "4. **Post-processing**: The model's output might be further processed to convert it into a more interpretable or usable format, depending on the application.\n",
    "\n",
    "In contrast, end-to-end learning aims to minimize the need for these intermediate steps by learning a direct mapping between the raw inputs and the desired outputs. This is achieved by training a single model to perform the entire task, which can involve complex transformations and feature extraction. Deep learning models, such as neural networks, are particularly well-suited for end-to-end learning due to their ability to learn hierarchical representations of data.\n",
    "\n",
    "#### Advantages of End-to-End Learning:\n",
    "\n",
    "1. **Automatic feature learning**: End-to-end learning eliminates the need for manual feature engineering, as the model learns to extract relevant features from raw data automatically. This can be particularly advantageous in domains where expert knowledge is scarce or difficult to encode.\n",
    "2. **Simplified learning pipeline**: By training a single model to perform the entire task, the overall learning pipeline is simplified, reducing the potential for errors and inconsistencies between different stages.\n",
    "3. **Potential for better performance**: In some cases, end-to-end learning can lead to better performance, as the model can learn task-specific representations of the data that might not be captured by hand-engineered features.\n",
    "\n",
    "#### Disadvantages of End-to-End Learning:\n",
    "\n",
    "1. **Large amounts of labeled data**: End-to-end learning often requires large amounts of labeled data to train effectively, as the model needs to learn complex mappings between raw inputs and outputs.\n",
    "2. **Computational requirements**: Training end-to-end models can be computationally intensive, particularly for deep learning models, which often require specialized hardware like GPUs or TPUs.\n",
    "3. **Lack of interpretability**: The features learned by end-to-end models can be difficult to interpret, making it harder to understand the model's decision-making process or diagnose errors.\n",
    "\n",
    "Examples of end-to-end learning can be found in various domains, such as automatic speech recognition (ASR) systems, where raw audio signals are mapped directly to text, and neural machine translation (NMT), where models learn to translate sentences from one language to another without relying on explicit intermediate representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-End Learning in Neural Machine Translation (NMT)\n",
    "\n",
    "Neural Machine Translation (NMT) is a prime example of end-to-end learning in natural language processing. NMT systems translate text from one language (source) to another (target) using deep learning models, specifically sequence-to-sequence (seq2seq) architectures. In contrast to earlier approaches like Statistical Machine Translation (SMT), which relied on multiple components, hand-crafted features, and intermediate steps, NMT simplifies the translation pipeline by learning a direct mapping between the source and target languages.\n",
    "\n",
    "#### NMT Model Architecture:\n",
    "\n",
    "A typical NMT system is based on a seq2seq model with an encoder-decoder architecture:\n",
    "\n",
    "1. **Encoder**: The encoder is responsible for processing the input text in the source language. It usually consists of a recurrent neural network (RNN), such as an LSTM or GRU, or more recently, a Transformer architecture. The encoder reads the input tokens one by one and generates a continuous, fixed-size representation (context vector) that captures the meaning of the entire input sequence.\n",
    "2. **Decoder**: The decoder takes the context vector generated by the encoder and generates the output text in the target language, token by token. Like the encoder, the decoder is typically an RNN or Transformer architecture. The decoder generates the output sequence by predicting the next token in the target language, given the context vector and the previously generated tokens.\n",
    "3. **Attention Mechanism**: An optional, but often crucial, component of the NMT system is the attention mechanism. Attention allows the decoder to focus on different parts of the input sequence during the translation process, enabling it to handle long-range dependencies more effectively. The attention mechanism computes a weighted sum of the encoder's hidden states, allowing the decoder to have a dynamic, context-dependent view of the input sequence.\n",
    "\n",
    "#### Advantages of End-to-End Learning in NMT:\n",
    "\n",
    "1. **Simplified pipeline**: NMT systems replace the complex and modular pipeline of SMT with a single, end-to-end trainable model, simplifying the overall translation process and reducing the potential for errors.\n",
    "2. **Automatic feature learning**: NMT models learn to extract relevant features and representations of the input text automatically, eliminating the need for manual feature engineering or domain-specific knowledge.\n",
    "3. **Improved translation quality**: NMT systems have been shown to outperform traditional SMT approaches in terms of translation quality, particularly for long and complex sentences, thanks to their ability to learn better representations of the input text and handle long-range dependencies more effectively.\n",
    "\n",
    "Despite these advantages, NMT systems also have some limitations, such as requiring large amounts of parallel, sentence-aligned corpora for training, and being computationally intensive. However, the success of end-to-end learning in NMT has inspired the adoption of similar approaches in other natural language processing tasks, such as text summarization, sentiment analysis, and dialogue systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Functions in Neural Networks\n",
    "\n",
    "Non-linear activation functions play a crucial role in neural networks for several reasons:\n",
    "\n",
    "1. **Capturing complex relationships**: Real-world data often contains complex, non-linear relationships between input features and output variables. By using non-linear activation functions, neural networks can learn to approximate these non-linear functions, allowing them to capture intricate patterns and make accurate predictions for a wide range of tasks.\n",
    "\n",
    "2. **Stacking layers**: In the absence of non-linear activation functions, a neural network would essentially act as a linear function of its inputs, regardless of the number of layers it has. This is because the composition of linear functions is still linear. The non-linear activation functions break this linearity, enabling the network to learn hierarchical representations of the input data. This is particularly important in deep learning, where multiple hidden layers are used to learn increasingly abstract features and representations from raw data.\n",
    "\n",
    "3. **Expressive power**: The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of $\\mathbb{R}^n$, given a suitable non-linear activation function. This expressive power is one of the primary reasons for the success of neural networks in a wide variety of applications. While the theorem holds for a single-layer network, in practice, deep networks with multiple hidden layers are more efficient in learning complex functions due to their ability to learn hierarchical representations.\n",
    "\n",
    "4. **Gradient-based learning**: Non-linear activation functions are used in conjunction with gradient-based optimization algorithms, such as stochastic gradient descent (SGD) and its variants. The gradients of these activation functions are crucial for updating the weights and biases during backpropagation. Activation functions like ReLU and its variants help mitigate the vanishing gradient problem, enabling deeper networks to learn effectively.\n",
    "\n",
    "5. **Sparsity**: Some non-linear activation functions, such as ReLU, introduce sparsity in the network's activations by setting a portion of the neuron outputs to zero. This sparsity can help reduce the model's complexity and improve its generalization capabilities by promoting the use of a smaller subset of neurons for specific tasks, effectively reducing overfitting.\n",
    "\n",
    "### Common Non-Linear Activation Functions\n",
    "\n",
    "1. **Sigmoid**: The sigmoid function is defined as $f(x) = \\frac{1}{1 + e^{-x}}$. It has an S-shaped curve and maps input values to the range (0, 1). It is commonly used in the output layer of binary classification tasks.\n",
    "\n",
    "2. **Tanh (Hyperbolic Tangent)**: The tanh function is defined as $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$. It has a similar S-shaped curve as the sigmoid function but maps input values to the range (-1, 1). It is often used in hidden layers of neural networks.\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit)**: The ReLU function is defined as $f(x) = \\max(0, x)$. It has a piecewise linear shape, mapping negative input values to 0 and preserving positive input values. ReLU is widely used in hidden layers of neural networks due to its computational efficiency and ability to mitigate the vanishing gradient problem.\n",
    "\n",
    "4. **Leaky ReLU**: The Leaky ReLU function is defined as $f(x) = \\max(\\alpha x, x)$, where $\\alpha$ is a small constant (e.g., 0.01). It is a modified version of ReLU that allows a small gradient for negative input values, helping to alleviate the dying ReLU problem, where some neurons can become inactive during training.\n",
    "\n",
    "5. **Softmax**: The softmaxfunction is used primarily in the output layer of multi-class classification tasks. It maps a vector of input values to a probability distribution over multiple classes. The softmax function is defined as $f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}}$, where $K$ is the number of classes.\n",
    "\n",
    "In summary, non-linear activation functions are essential for neural networks to learn complex, non-linear relationships between inputs and outputs, enable the stacking of multiple layers to form deep architectures, provide expressive power, support gradient-based learning, and promote sparsity in the activations. These properties together contribute to the success of neural networks in various applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of Non-Linear Functions in Neural Networks\n",
    "\n",
    "Non-linear activation functions play a crucial role in neural networks for several reasons:\n",
    "\n",
    "1. **Capturing complex relationships**: Real-world data often contains complex, non-linear relationships between input features and output variables. By using non-linear activation functions, neural networks can learn to approximate these non-linear functions, allowing them to capture intricate patterns and make accurate predictions for a wide range of tasks.\n",
    "\n",
    "2. **Stacking layers**: In the absence of non-linear activation functions, a neural network would essentially act as a linear function of its inputs, regardless of the number of layers it has. This is because the composition of linear functions is still linear. The non-linear activation functions break this linearity, enabling the network to learn hierarchical representations of the input data. This is particularly important in deep learning, where multiple hidden layers are used to learn increasingly abstract features and representations from raw data.\n",
    "\n",
    "3. **Expressive power**: The Universal Approximation Theorem states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of $\\mathbb{R}^n$, given a suitable non-linear activation function. This expressive power is one of the primary reasons for the success of neural networks in a wide variety of applications. While the theorem holds for a single-layer network, in practice, deep networks with multiple hidden layers are more efficient in learning complex functions due to their ability to learn hierarchical representations.\n",
    "\n",
    "4. **Gradient-based learning**: Non-linear activation functions are used in conjunction with gradient-based optimization algorithms, such as stochastic gradient descent (SGD) and its variants. The gradients of these activation functions are crucial for updating the weights and biases during backpropagation. Activation functions like ReLU and its variants help mitigate the vanishing gradient problem, enabling deeper networks to learn effectively.\n",
    "\n",
    "5. **Sparsity**: Some non-linear activation functions, such as ReLU, introduce sparsity in the network's activations by setting a portion of the neuron outputs to zero. This sparsity can help reduce the model's complexity and improve its generalization capabilities by promoting the use of a smaller subset of neurons for specific tasks, effectively reducing overfitting.\n",
    "\n",
    "In summary, non-linear activation functions are essential for neural networks to learn complex, non-linear relationships between inputs and outputs, enable the stacking of multiple layers to form deep architectures, provide expressive power, support gradient-based learning, and promote sparsity in the activations. These properties together contribute to the success of neural networks in various applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the example of predicting house prices based on various features such as the size of the house, the number of bedrooms, the age of the house, and the location. The relationship between these features and the final price of a house is often non-linear.\n",
    "\n",
    "For instance, the price of a house may increase disproportionately with its size, exhibiting a non-linear relationship. Similarly, the relationship between the age of a house and its price could be non-linear, with the price decreasing rapidly for very old houses and then stabilizing for newer ones. Furthermore, the impact of location on the price could be non-linear as well, with certain prime locations commanding a premium that is not directly proportional to the distance from city centers or other amenities.\n",
    "\n",
    "A neural network with non-linear activation functions can model these non-linear relationships by learning complex, hierarchical representations of the input features. By using non-linear functions such as ReLU or tanh, the network can capture the intricate patterns in the data, allowing it to make accurate predictions for house prices based on the given features.\n",
    "\n",
    "In this scenario, the input features (size, number of bedrooms, age, and location) would be fed into the neural network, which would then use its hidden layers and non-linear activation functions to learn the underlying non-linear relationship between these features and the target variable (house price). Once trained, the neural network would be able to predict the price of a house for a given set of features, even if the relationship between those features and the price is non-linear and complex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Back Propagation ##\n",
    "Backpropagation is a supervised learning algorithm used to train feedforward artificial neural networks, including deep learning models like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). It's a type of supervised learning because it requires labeled training data to adjust the model's parameters (weights and biases) to minimize the error between the predicted outputs and the true outputs (labels).\n",
    "\n",
    "The backpropagation algorithm consists of two main steps: forward pass and backward pass.\n",
    "\n",
    "1. Forward pass: During the forward pass, the input data is propagated through the network layer by layer, starting from the input layer, passing through the hidden layers, and ending at the output layer. At each layer, the input is multiplied by the layer's weights, the biases are added, and the result is passed through an activation function to produce the output for the next layer. Once the forward pass is complete, the predicted output is compared to the true output, and a loss function is used to compute the error.\n",
    "\n",
    "2. Backward pass: In the backward pass, the error is propagated backward through the network to update the weights and biases. This is done by computing the gradient of the loss function with respect to each weight and bias using the chain rule, which allows us to find the rate of change of the loss function concerning the parameters.\n",
    "\n",
    "The chain rule is a fundamental concept in calculus that states that the derivative of a composite function is the product of the derivatives of its constituent functions. In the context of backpropagation, the chain rule is used to compute the gradients of the loss function with respect to the weights and biases in the network by decomposing the derivative into a product of simpler derivatives.\n",
    "\n",
    "For example, let's consider a simple feedforward neural network with one hidden layer. To compute the gradient of the loss function with respect to the weights in the hidden layer, we need to find how the loss function changes with respect to the output of the hidden layer, how the output of the hidden layer changes with respect to its input (weighted sum of the previous layer), and how the input of the hidden layer changes with respect to the weights. Using the chain rule, we multiply these partial derivatives to obtain the gradient of the loss function with respect to the weights in the hidden layer.\n",
    "\n",
    "Once the gradients are computed, an optimization algorithm like stochastic gradient descent (SGD) or one of its variants (e.g., Adam, RMSprop) is used to update the weights and biases in the network. This process of forward pass, backward pass, and weight updates is repeated for multiple iterations (epochs) until the model converges, and the error is minimized.\n",
    "\n",
    "In summary, backpropagation is a supervised learning algorithm that trains neural networks by minimizing the error between predicted outputs and true outputs. It involves a forward pass to compute the predicted output and a backward pass to compute the gradients of the loss function with respect to the model's parameters using the chain rule. These gradients are then used to update the weights and biases in the network, improving its performance on the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding ###\n",
    "One-hot encoding is a technique used to represent words as numerical vectors in a way that is easy for computers to understand. In one-hot encoding, each word in the vocabulary is represented by a vector where all elements are 0, except for a single element which is 1. The position of the 1 in the vector is unique for each word.\n",
    "\n",
    "For example, let's say you have a small vocabulary consisting of the words from the sentence \"The cat sat on the mat.\" You can assign a unique index to each word:\n",
    "\n",
    "1. The\n",
    "2. cat\n",
    "3. sat\n",
    "4. on\n",
    "5. the\n",
    "6. mat\n",
    "\n",
    "Now, you can create one-hot encoded vectors for each word:\n",
    "\n",
    "1. The: [1, 0, 0, 0, 0, 0]\n",
    "2. cat: [0, 1, 0, 0, 0, 0]\n",
    "3. sat: [0, 0, 1, 0, 0, 0]\n",
    "4. on:  [0, 0, 0, 1, 0, 0]\n",
    "5. the: [0, 0, 0, 0, 1, 0]\n",
    "6. mat: [0, 0, 0, 0, 0, 1]\n",
    "\n",
    "Each word is represented by a vector with a 1 in the position corresponding to its unique index and 0s in all other positions. This is called one-hot encoding, and it allows computers to process words as numerical data, making it easier to work with text in machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the loss function for transformer neural networks ##\n",
    "\n",
    "The choice of the loss function for transformers depends on the specific task being solved. However, a common loss function used for many tasks, including sequence-to-sequence tasks such as translation or text generation, is the cross-entropy loss. \n",
    "\n",
    "The cross-entropy loss compares the probability distribution produced by the transformer (the output predictions) with the actual target probability distribution (the ground truth). For multi-class classification problems, the ground truth is typically represented as a one-hot encoded vector.\n",
    "\n",
    "Mathematically, the cross-entropy loss for a single data point can be defined as:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\sum_{i=1}^N y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Where $N$ is the number of classes (or vocabulary size for natural language processing tasks), $y_i$ is the ground truth probability for class $i$, and $\\hat{y}_i$ is the predicted probability for class $i$.\n",
    "\n",
    "For a batch of data points, the average cross-entropy loss is used:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\frac{1}{M} \\sum_{j=1}^M \\sum_{i=1}^N y_{ji} \\log(\\hat{y}_{ji})\n",
    "$$\n",
    "\n",
    "Where $M$ is the number of data points in the batch.\n",
    "\n",
    "In the context of transformers, the loss is computed for each position in the output sequence and then averaged over the whole sequence or batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is cross-entropy ##\n",
    "Cross-entropy is a measure of the difference between two probability distributions. In the context of machine learning and deep learning, it is commonly used as a loss function to quantify the difference between the predicted probability distribution and the actual (ground truth) distribution of the target variable. The goal of the learning process is to minimize the cross-entropy loss, which essentially means making the predicted probabilities as close as possible to the ground truth probabilities.\n",
    "\n",
    "The cross-entropy loss for a single data point can be defined as:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\sum_{i=1}^N y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the number of classes.\n",
    "- $y_i$ represents the ground truth probability for class $i$. In most cases, this is a one-hot encoded vector where the correct class has a probability of 1 and all other classes have a probability of 0.\n",
    "- $\\hat{y}_i$ is the predicted probability for class $i$.\n",
    "\n",
    "The cross-entropy loss is a sum of the products of the ground truth probabilities and the logarithm of the predicted probabilities for each class. In practice, since the ground truth probabilities are one-hot encoded, only the term corresponding to the correct class contributes to the sum, as all other terms will be zero.\n",
    "\n",
    "The cross-entropy loss has some desirable properties:\n",
    "\n",
    "1. It is always non-negative. A smaller value indicates that the predicted probabilities are closer to the ground truth probabilities, while a larger value indicates a greater difference between the predicted and ground truth probabilities.\n",
    "2. It is minimized when the predicted probabilities are equal to the ground truth probabilities. In this case, the cross-entropy loss is equal to the entropy of the ground truth distribution, which is the minimum possible value.\n",
    "3. It is differentiable, making it suitable for gradient-based optimization algorithms like stochastic gradient descent.\n",
    "\n",
    "In summary, the cross-entropy function measures the dissimilarity between two probability distributions, and it is widely used as a loss function in machine learning and deep learning to train models to produce accurate probability estimates for classification tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
